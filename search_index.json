[
["index.html", "Advanced Data Analysis from an Elementary Point of View 中文笔记 1 前言", " Advanced Data Analysis from an Elementary Point of View 中文笔记 于淼 1 前言 这是对一本目前未正式出版的教材的笔记。 "],
["section-2.html", "2 回归：预测与相关定量特征 2.1 统计学 数据分析与回归 2.2 均值回归 2.3 回归方程 2.4 估计回归方程 2.5 线性平滑", " 2 回归：预测与相关定量特征 2.1 统计学 数据分析与回归 Statistics is the branch of mathematical engineering which designs and analyses methods for drawing reliable inferences from imperfect data. 统计学不研究世界而研究我们理解世界的方法 科学都是在处理不完美的数据 人们会对两个或多个变量间关系感兴趣而回归最常使用 回归通常通过预测来验证 回归是数据分析、推断及预测的基本出发点 2.2 均值回归 首先猜一个随机数量的值，判断预测好坏要通过与真实值的比对，于是有如下函数表示预测误差： \\(MSE(\\alpha) \\equiv E[(Y - \\alpha)^2]\\) 这里使用了均方误，使这个函数最小有: \\(MSE(\\alpha) = (E[Y]-\\alpha)^2 + Var[Y]\\) 对\\(\\alpha\\)求导，可得当\\(\\alpha = E[Y]\\)时预测误差最小。 使用均方误表示预测误差的函数，最佳预测点为\\(Y\\)的均值，用\\(r\\)表示。 2.2.1 估计期望值 采样，\\(Y\\)的期望估计为： \\(\\hat r \\equiv \\frac{1}{n} \\sum_i^n y_i\\) 当样本为IID时，有 \\(\\hat r \\rightarrow E[Y] = r\\) 大数定律表明这个均值可通过对样本取均值获得，收敛速度与取样数跟总体方差与样本数有关。 sit, wait, and average 2.3 回归方程 通常预测不是瞎猜，如果知道有其他数量与要预测的量有关系，可以通过这些自变量去预测因变量。 预测为一种条件概率形式： \\(E[U] = E[E[U|V]]\\) 这样我们可以把\\(Y\\)写成\\(f(x)\\)的形式，有： \\(MSE(f(x)) = E[Var[Y|X]+(E[Y - f(x)|X])^2]\\) 因为预测只依赖\\(f(x)\\)，所以有需优化的回归函数: \\(r(x) = E[Y|X = x]\\) 因此\\(r(x)\\)可理解为条件期望的回归函数。 2.3.1 声明 当使用自变量与因变量术语时，默认存在因果模型： \\(Y \\leftarrow r(X) + \\epsilon\\) 其中默认\\(\\epsilon\\)为具有固定分布独立于\\(X\\)的误差项，但实际过程无法假定其独立，有： \\(Y|X = r(X) + \\eta(X)\\) \\(\\eta(X)\\)是均值0的噪音项，依赖于\\(X\\)。 即使\\(r(X)\\)为常数也不说明\\(X\\)与\\(Y\\)是独立的，即达到相同的结果不说明原因相同或不同，确认偏误。 2.4 估计回归方程 如果\\(X\\)无限存在，那么预测\\(Y\\)可以通过条件样本平均值，在给定大量\\(X\\)下求\\(Y\\)的均值： \\(\\hat r(x) = \\frac{1}{i:x_i = x} \\sum_{i:x_i = x} y_i\\) 大数定理下 \\(\\hat r(x) \\rightarrow E[Y|X=x]\\) 但现实状况是预测总是基于已有\\(X\\)来考虑\\(Y\\)，这就存在两个问题： \\(X\\)不够采样导致预测存在偏差 关系能否外推并平滑 进行模型预测时，实际上是对已知自变量因变量关系的内插，外推与平滑。 2.4.1 Bias-Variance Tradeoff 模型将实际过程的自变量限制到一种与因变量的关系里，这个关系是对真实关系的近似。在\\(x\\)点有 \\(MSE(\\hat r(x)) = \\sigma_x^2 + ((r(x) - \\hat r(x))^2\\) 前者为\\(x\\)作为预测形成的统计扰动，后者是实际预测与真实预测的偏差。 另一个\\(MSE\\)来自实际预测的模型受到有限样本的条件限制为\\(MSE(\\hat R_n (x)| \\hat R_n = \\hat r)\\) 有 \\(MSE(\\hat R_n(x)) = \\sigma_x^2 + (r(x) - E[\\hat R_n(x)])^2 + Var[\\hat R_n(x)]\\) 第一项为\\(x\\)预测形成的统计扰动，第二项是估计偏差，第三项是估计模型的变动。 也就是说，在特定的点上，降低估计偏差会提高估计模型的变动，也就是bias-variance trade-off。因为偏差与统计模型降低的速度不一，我们可以期望得到一个\\(MSE\\)最小的建模估计。 拟合越精细，偏差越小，但模型受\\(x\\)影响变动大，在预测上受影响大。 Figure 2.1: 红线表示均值，蓝线表示按原函数形式拟合的方程，黑线表示原函数，均值得到的均方误要小于函数拟合形式，该图说明我们可以牺牲准确性来得到均方误小的模型 2.4.2 最小二乘拟合 假设\\(X\\)与\\(Y\\)都是均值为0的两组变量，最小二乘回归是最小化均方误，也就是 \\(MSE(\\alpha,\\beta) = E[(Y-\\alpha-\\beta X)^2|X] = E[Var[Y|X]]+E[(E[Y-\\alpha-\\beta X|X])^2]\\) 前面一项无参数，对后面微分，可得 \\(\\frac{\\partial MSE}{\\partial \\alpha} = E[2(Y-\\alpha-\\beta X)(-1)]\\) 即： \\(\\alpha = E[Y] - b E[X] = 0\\) \\(\\frac{\\partial MSE}{\\partial \\beta} = E[2(Y - \\alpha - \\beta X)(-X)]\\) 即： \\(\\beta = \\frac{Cov[X,Y]}{Var[X]}\\) 预测函数为： \\(r(x) = x \\frac{Cov[X,Y]}{Var[X]}\\) 给定一组样本，有： \\(RSS(\\alpha,\\beta) \\equiv \\sum_i(y_i - \\alpha - \\beta x_i)^2\\) 解得： \\(\\hat a = 0\\) \\(\\hat b = \\frac{\\sum_i y_i x_i}{\\sum_i x_i^2}\\) \\(\\hat r(x) = \\sum_i y_i \\frac{x_i}{n s_X^2}x\\) 这可看作对样本\\(y_i\\)的加权平均，权重依赖\\(x_i\\)距离中心的距离，正比于\\(x\\)的数量级，反比于样本方差\\(s_X^2\\). 最小二乘回归的本质可看作训练集中对因变量的中心自变量加权平滑。 2.5 线性平滑 均值回归与最小二乘回归是线性平滑的特例 \\(\\hat r(x)=\\sum_i y_i \\hat \\omega (x_i,x)\\) 均值是\\(\\hat \\omega(x_i,x) = 1/n\\) OLS是\\(\\hat \\omega(x_i,x) = (x_i/ns_X^2)x\\) 核平滑又称核回归或Nadaraya-Watson回归，需要核函数\\(K(x_i,x)\\)，核函数要满足下面特性： \\(K(x_i,x)\\geq 0\\) \\(K(x_i,x)\\) 只依赖于距离\\(x_i - x\\) \\(\\int x K(0,x) dx = 0\\) \\(0 &lt; \\int x^2 K(0,x) dx &lt; \\infty\\) 回归函数形式： \\(\\hat r(x) = \\sum_i y_i \\frac{K(x_i,x)}{\\sum_jK(x_j,x)}\\) 也就是： \\(\\hat \\omega(x_i,x) = \\frac{K(x_i,x)}{\\sum_jK(x_j,x)}\\) 带宽\\(b\\)，高斯核函数： \\(e^{-(x-x_i)^2/2b}\\) 核函数保证靠近预测点权重高，远的降低但不会为0. "],
["section-3.html", "3 线性回归的真相 3.1 多变量回归 3.2 分布变动 缺失变量与转化", " 3 线性回归的真相 3.1 多变量回归 回归函数\\(r(\\vec x)\\)可用Taylor展开式展开： \\(r(\\vec x) = r(\\vec u) + (\\vec x - \\vec u) \\cdot \\nabla r(\\vec u) + O(||\\vec x - \\vec u||^2)\\) 求解其参数\\(\\beta\\)： \\(\\beta = v^{-1} Cov [\\vec X, Y]\\) \\(v\\)是\\(\\bar X\\)的协方差矩阵，\\(Cov [\\vec X, Y]\\)是自变量与因变量的协方差矩阵。 自变量没有共线性时，每个自变量的系数可以单独求加起来，有的话考虑协方差矩阵。 3.1.1 共线性 共线性会导致\\(v\\)不可逆，不要在自变量选择上出现线性组合。 3.1.2 预测残差 \\(Cov[Y - \\vec X \\cdot \\beta] = Cov[Y, \\vec X] - Cov[\\vec X \\cdot (v^{-1} Cov[\\vec X, Y]), \\vec X] = Cov[Y, \\vec X] - v v^{-1} Cov[Y, \\vec X] = 0\\) 预测残差对整体\\(\\vec X\\)可以是0，对特定\\(\\vec x\\)预测残差不是0。 3.1.3 估计最佳线性预测函数 大数法则，样本协方差会逼近真实协方差，有： \\(\\hat \\beta = (x^T x)^{-1} x^T y \\rightarrow \\beta\\) 或者最小化残差平方和： \\(RSS(\\beta) \\equiv \\sum_{i=1}^n (y_i - \\vec x_i \\cdot \\beta)^2\\) 最小化也需要协方差但不需要概率假设，但考虑参数\\(\\hat \\beta\\)的收敛需要不变的协方差概率分布假设。 3.1.3.1 最小二乘估计的无偏性与方差 考虑\\(x\\)固定，\\(Y\\)是随机的，有： \\(Y = \\vec X \\cdot \\beta + \\epsilon\\) 其中\\(E[\\epsilon] = 0\\)，\\(Cov[\\epsilon, \\vec X] = 0\\) 可得参数\\(\\beta\\)为： \\(\\hat \\beta = (x^T x)^{-1} x^T Y = \\beta + (x^T x)^{-1} x^T \\epsilon\\) 这告诉我们\\(\\hat \\beta\\)是无偏的： \\(E[\\hat \\beta|x] = \\beta + (x^T x)^{-1} x^T E[\\epsilon] = \\beta + 0 = \\beta\\) 也可得到\\(\\hat \\beta\\)的方差： \\(Var[\\hat \\beta| x ] = Var[\\beta + (x^T x)^{-1} x^T \\epsilon |x] = Var[(x^T x)^{-1} x^T \\epsilon | x] = E[((x^T x)^{-1} x^T \\epsilon)^2 | x] = (x^T x)^{-1} x^T Var[\\epsilon|x] x (x^T x)^{-1}\\) 当\\(\\epsilon\\)等方差为\\(\\sigma^2\\)时，有： \\(Var[\\hat \\beta|x] = \\sigma^2 (x^T x)^{-1}\\) 即： \\(Var[\\hat \\beta] = \\frac{\\sigma^2}{n} v^{-1}\\) 系数\\(\\hat \\beta\\)的方差会随样本数上升而下降，线性变差而变大，自变量协方差变大（不相关）而变小。 3.2 分布变动 缺失变量与转化 有些模型非线性但可得到较高\\(R^2\\). 缺失的潜在变量如果与已知变量相关会影响自变量对因变量的回归。例如\\(Y \\sim \\mathcal N(X+Z,0.01)\\), 如果\\(X\\)与\\(Z\\)正相关，\\(Y\\)预测会偏高，反之\\(Y\\)回归均值。这一变化是因为潜在变量的存在而不是其与\\(Y\\)的回归关系。 有时变量是另一个变量的扭曲\\(\\vec X = \\vec U + \\vec \\eta\\)，\\(\\eta\\)是某种噪音，这会导致估计的参数偏小，不过不用钻牛角尖，真实数据总是这样。 数据转化可同时操作\\(X\\)与\\(Y\\)，但推荐在\\(X\\)上： \\(E[f(Y)] \\neq f(E[Y])\\) 使用最小二乘法会产生偏差 自变量间的转化形式可以不同 可以使用复杂形式的协变量 "]
]
