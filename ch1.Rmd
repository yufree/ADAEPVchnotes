回归
========================================================

回归是数据分析、推断及预测的基本出发点。

首先猜一个随机数量的值，如果没有任何其他信息，判断预测好坏要通过与真实值的比对，于是有如下函数表示预测误差：

$MSE(\alpha) \equiv E[(Y - \alpha)^2]$

这里使用了均方误，使这个函数最小有:

$MSE(\alpha) = (E[Y]-\alpha)^2 + Var[Y]$

对$\alpha$求导，可得当$\alpha = E[Y]$时预测误差最小。换言之，使用均方误表示预测误差的函数，最佳预测点为$Y$的均值。

当样本为IID时，大数定律表明这个均值可通过对样本取均值获得，收敛速度与取样数跟总体方差有关。

如果知道有其他数量与要预测的量有关系，可以通过这些自变量去预测因变量，背后关系为一种条件概率形式：

$E[U] = E[E[U|V]]$

这样我们可以把$Y$写成$f(x)$的形式，有：

$MSE(f(x)) = E[Var[Y|X]+(E[Y - f(x)|X])^2]$

因为预测只依赖$f(x)$，所以有最佳预测:

$r(x) = E[Y|X = x]$

因此$r(x)$可理解为条件期望的回归函数。

当使用自变量与因变量术语时，默认存在因果模型：

$Y \leftarrow \gamma(X) + \epsilon$

其中默认$\epsilon$为具有固定分布的误差项，但实际过程无法假定两个变量存在关系，有：

$Y|X = \gamma(X) + \eta(X)$

$\eta(X)$是均值0的噪音项，依赖于$X$，同时即使$\gamma(X)$为常数也不说明没有关系。

如果$X$无限存在，那么预测$Y$可以通过条件样本平均值：

$\hat \gamma(x) = \frac{1}{i:x_i  = x} \sum_{i:x_i = x} y_i$

但现实状况是预测总是基于已有$X$来考虑$Y$，这就存在两个问题：$X$不够采样导致预测存在偏差；关系能否外推并平滑。

进行模型预测时，实际上是通过一种模型将实际过程的自变量限制到一种与因变量的关系里，这个关系是对真实关系的近似。在x点有

$MSE(\hat \gamma (x)) = \sigma_x^2 + ((\gamma(x) - \hat \gamma (x))^2$

前者为$x$作为预测形成的统计扰动，后者是实际预测与真实预测的偏差。

另一个$MSE$来自实际预测的模型受到有限样本的条件限制为$MSE(\hat R_n (x)| \hat R_n = \hat r)$ 有

$MSE(\hat R_n(x)) = \sigma_x^2 + (r(x) - E[\hat R_n(x)])^2 + Var[\hat R_n(x)]$

第一项为$x$预测形成的统计扰动，第二项是估计偏差，第三项是估计模型的变动。

也就是说，在特定的点上，降低估计偏差会提高估计模型的变动，也就是*bias-variance trade-off*。因为偏差与统计模型降低的速度不一，我们可以期望得到一个$MSE$最小的建模估计，拟合越精细，偏差越小，但模型受$x$影响变动大，在预测上受影响大。
最小二乘回归的本质是在训练集中对因变量的中心平滑。