---
title: "回归：预测与相关定量特征"
author: "于淼"
date: "2014年12月15日"
output:
  slidy_presentation:
    fig_caption: true
---

## 统计学 数据分析与回归

> Statistics is the branch of mathematical engineering which designs and analyses methods for drawing reliable inferences from imperfect data.

- 统计学不研究世界而研究我们理解世界的方法
- 科学都是在处理不完美的数据
- 人们会对两个或多个变量间关系感兴趣而回归最常使用
- 回归通常通过预测来验证

> 回归是数据分析、推断及预测的基本出发点

## 均值回归

首先猜一个随机数量的值，判断预测好坏要通过与真实值的比对，于是有如下函数表示预测误差：

$MSE(\alpha) \equiv E[(Y - \alpha)^2]$

这里使用了均方误，使这个函数最小有:

$MSE(\alpha) = (E[Y]-\alpha)^2 + Var[Y]$

对$\alpha$求导，可得当$\alpha = E[Y]$时预测误差最小。

使用均方误表示预测误差的函数，最佳预测点为$Y$的均值，用$r$表示。

### 估计期望值

采样，$Y$的期望估计为：

$\hat r \equiv \frac{1}{n} \sum_i^n y_i$

当样本为IID时，有

$\hat r \rightarrow E[Y] = r$

大数定律表明这个均值可通过对样本取均值获得，收敛速度与取样数跟总体方差与样本数有关。

> sit, wait, and average

## 回归方程

通常预测不是瞎猜，如果知道有其他数量与要预测的量有关系，可以通过这些自变量去预测因变量。

预测为一种条件概率形式：

$E[U] = E[E[U|V]]$

这样我们可以把$Y$写成$f(x)$的形式，有：

$MSE(f(x)) = E[Var[Y|X]+(E[Y - f(x)|X])^2]$

因为预测只依赖$f(x)$，所以有需优化的回归函数:

$r(x) = E[Y|X = x]$

因此$r(x)$可理解为条件期望的回归函数。

### 声明

当使用自变量与因变量术语时，默认存在因果模型：

$Y \leftarrow r(X) + \epsilon$

其中默认$\epsilon$为具有固定分布独立于$X$的误差项，但实际过程无法假定其独立，有：

$Y|X = r(X) + \eta(X)$

$\eta(X)$是均值0的噪音项，依赖于$X$。

即使$r(X)$为常数也不说明$X$与$Y$是独立的，即达到相同的结果不说明原因相同或不同，确认偏误。

## 估计回归方程

如果$X$无限存在，那么预测$Y$可以通过条件样本平均值，在给定大量$X$下求$Y$的均值：

$\hat r(x) = \frac{1}{i:x_i  = x} \sum_{i:x_i = x} y_i$

大数定理下 $\hat r(x) \rightarrow E[Y|X=x]$

但现实状况是预测总是基于已有$X$来考虑$Y$，这就存在两个问题：

- $X$不够采样导致预测存在偏差
- 关系能否外推并平滑

进行模型预测时，实际上是对已知自变量因变量关系的内插，外推与平滑。

### Bias-Variance Tradeoff

模型将实际过程的自变量限制到一种与因变量的关系里，这个关系是对真实关系的近似。在$x$点有

$MSE(\hat r(x)) = \sigma_x^2 + ((r(x) - \hat r(x))^2$

前者为$x$作为预测形成的统计扰动，后者是实际预测与真实预测的偏差。

另一个$MSE$来自实际预测的模型受到有限样本的条件限制为$MSE(\hat R_n (x)| \hat R_n = \hat r)$ 有

$MSE(\hat R_n(x)) = \sigma_x^2 + (r(x) - E[\hat R_n(x)])^2 + Var[\hat R_n(x)]$

第一项为$x$预测形成的统计扰动，第二项是估计偏差，第三项是估计模型的变动。

也就是说，在特定的点上，降低估计偏差会提高估计模型的变动，也就是*bias-variance trade-off*。因为偏差与统计模型降低的速度不一，我们可以期望得到一个$MSE$最小的建模估计。

拟合越精细，偏差越小，但模型受$x$影响变动大，在预测上受影响大。

```{r echo=FALSE,fig.cap='红线表示均值，蓝线表示按原函数形式拟合的方程，黑线表示原函数，均值得到的均方误要小于函数拟合形式，该图说明我们可以牺牲准确性来得到均方误小的模型'}
ugly.func = function(x) {1 + 0.01*sin(100*x)}
r = runif(100); y = ugly.func(r) + rnorm(length(r),0,0.5)
plot(r,y,xlab="x",ylab="y"); curve(ugly.func,add=TRUE)
abline(h=mean(y),col="red")
sine.fit = lm(y ~ 1+ sin(100*r))
curve(sine.fit$coefficients[1]+sine.fit$coefficients[2]*sin(100*x),
      col="blue",add=TRUE)
cat('\n\n') 
```

### 最小二乘拟合

假设$X$与$Y$都是均值为0的两组变量，最小二乘回归是最小化均方误，也就是

$MSE(\alpha,\beta) = E[(Y-\alpha-\beta X)^2|X] = E[Var[Y|X]]+E[(E[Y-\alpha-\beta X|X])^2]$

前面一项无参数，对后面微分，可得

$\frac{\partial MSE}{\partial \alpha} = E[2(Y-\alpha-\beta X)(-1)]$

即：

$\alpha = E[Y] - b E[X] = 0$

$\frac{\partial MSE}{\partial \beta} = E[2(Y - \alpha - \beta X)(-X)]$

即：

$\beta = \frac{Cov[X,Y]}{Var[X]}$

预测函数为：

$r(x) = x \frac{Cov[X,Y]}{Var[X]}$

给定一组样本，有：

$RSS(\alpha,\beta) \equiv \sum_i(y_i - \alpha - \beta x_i)^2$

解得：

$\hat a = 0$

$\hat b = \frac{\sum_i y_i x_i}{\sum_i x_i^2}$

$\hat r(x) = \sum_i y_i \frac{x_i}{n s_X^2}x$

这可看作对样本$y_i$的加权平均，权重依赖$x_i$距离中心的距离，正比于$x$的数量级，反比于样本方差$s_X^2$.

最小二乘回归的本质可看作训练集中对因变量的中心自变量加权平滑。

## 线性平滑

均值回归与最小二乘回归是线性平滑的特例

$\hat r(x)=\sum_i y_i \hat \omega (x_i,x)$

均值是$\hat \omega(x_i,x) = 1/n$ OLS是$\hat \omega(x_i,x) = (x_i/ns_X^2)x$

核平滑又称核回归或Nadaraya-Watson回归，需要核函数$K(x_i,x)$，核函数要满足下面特性：

- $K(x_i,x)\geq 0$
- $K(x_i,x)$ 只依赖于距离$x_i - x$
- $\int x K(0,x) dx = 0$
- $0 < \int x^2 K(0,x) dx < \infty$

回归函数形式：

$\hat r(x) = \sum_i y_i \frac{K(x_i,x)}{\sum_jK(x_j,x)}$

也就是：

$\hat \omega(x_i,x) = \frac{K(x_i,x)}{\sum_jK(x_j,x)}$

带宽$b$，高斯核函数：

$e^{-(x-x_i)^2/2b}$

核函数保证靠近预测点权重高，远的降低但不会为0.