---
title: "回归：预测与相关定量特征"
author: "于淼"
date: "2014年12月15日"
output: slidy_presentation
---

## 统计学 数据分析与回归

> Statistics is the branch of mathematical engineering which designs and analyses methods for drawing reliable inferences from imperfect data.

- 统计学不研究世界而研究我们理解世界的方法
- 科学都是在处理不完美的数据
- 人们会对两个或多个变量间关系感兴趣而回归最常使用
- 回归通常通过预测来验证

> 回归是数据分析、推断及预测的基本出发点

## 均值回归

首先猜一个随机数量的值，判断预测好坏要通过与真实值的比对，于是有如下函数表示预测误差：

$MSE(\alpha) \equiv E[(Y - \alpha)^2]$

这里使用了均方误，使这个函数最小有:

$MSE(\alpha) = (E[Y]-\alpha)^2 + Var[Y]$

对$\alpha$求导，可得当$\alpha = E[Y]$时预测误差最小。

使用均方误表示预测误差的函数，最佳预测点为$Y$的均值，用$r$表示。

### 估计期望值

采样，$Y$的期望估计为：

$\hat r \equiv \frac{1}{n} \sum_i^n y_i$

当样本为IID时，有

$\hat r \rightarrow E[Y] = r$

大数定律表明这个均值可通过对样本取均值获得，收敛速度与取样数跟总体方差与样本数有关。

> sit, wait, and average

## 回归方程

通常预测不是瞎猜，如果知道有其他数量与要预测的量有关系，可以通过这些自变量去预测因变量。

预测为一种条件概率形式：

$E[U] = E[E[U|V]]$

这样我们可以把$Y$写成$f(x)$的形式，有：

$MSE(f(x)) = E[Var[Y|X]+(E[Y - f(x)|X])^2]$

因为预测只依赖$f(x)$，所以有需优化的回归函数:

$r(x) = E[Y|X = x]$

因此$r(x)$可理解为条件期望的回归函数。

### 声明

当使用自变量与因变量术语时，默认存在因果模型：

$Y \leftarrow r(X) + \epsilon$

其中默认$\epsilon$为具有固定分布独立于$X$的误差项，但实际过程无法假定其独立，有：

$Y|X = r(X) + \eta(X)$

$\eta(X)$是均值0的噪音项，依赖于$X$。

即使$r(X)$为常数也不说明$X$与$Y$是独立的，即达到相同的结果不说明原因相同或不同，确认偏误。

## 估计回归方程

如果$X$无限存在，那么预测$Y$可以通过条件样本平均值，在给定大量$X$下求$Y$的均值：

$\hat r(x) = \frac{1}{i:x_i  = x} \sum_{i:x_i = x} y_i$

大数定理下 $\hat \gamma(x) \rightarrow E[Y|X=x]$

但现实状况是预测总是基于已有$X$来考虑$Y$，这就存在两个问题：

- $X$不够采样导致预测存在偏差
- 关系能否外推并平滑

进行模型预测时，实际上是对已知自变量因变量关系的内插，外推与平滑。

### Bias-Variance Tradeoff

模型将实际过程的自变量限制到一种与因变量的关系里，这个关系是对真实关系的近似。在$x$点有

$MSE(\hat r(x)) = \sigma_x^2 + ((r(x) - \hat r(x))^2$

前者为$x$作为预测形成的统计扰动，后者是实际预测与真实预测的偏差。

另一个$MSE$来自实际预测的模型受到有限样本的条件限制为$MSE(\hat R_n (x)| \hat R_n = \hat \gamma)$ 有

$MSE(\hat R_n(x)) = \sigma_x^2 + (\gamma(x) - E[\hat R_n(x)])^2 + Var[\hat R_n(x)]$

第一项为$x$预测形成的统计扰动，第二项是估计偏差，第三项是估计模型的变动。

也就是说，在特定的点上，降低估计偏差会提高估计模型的变动，也就是*bias-variance trade-off*。因为偏差与统计模型降低的速度不一，我们可以期望得到一个$MSE$最小的建模估计，拟合越精细，偏差越小，但模型受$x$影响变动大，在预测上受影响大。

最小二乘回归是最小化均方误，也就是

$MSE(\alpha,\beta) = E[(Y-\alpha-\beta X)^2|X] = E[Var[Y|X]]+E[(E[Y-\alpha-\beta X|X])^2]$

前面一项无参数，对后面微分，可得

$\frac{\partial MSE}{\partial \alpha} = E[2(Y-\alpha-\beta X)(-1)]$

$\frac{\partial MSE}{\partial \beta} = E[2]$
最小二乘回归的本质是在训练集中对因变量的中心平滑。

## 线性平滑

均值回归与最小二乘回归是线性平滑的特例

$\hat r(x)=\sum_i y_i \hat \omega (x_i,x)$

均值是$\hat \omega(x_i,x) = 1/n$ OLS是$\hat \omega(x_i,x) = (x_i/ns_X^2)x$

核平滑又称核回归或Nadaraya-Watson回归，需要核函数$K(x_i,x)$，核函数要满足下面特性：

- $K(x_i,x)\geq 0$
- $K(x_i,x)$ 只依赖于距离$x_i - x$
- $\int x K(0,x) dx = 0$
- $0 < \int x^2 K(0,x) dx < \infty$

回归函数形式：

$\hat r(x) = \sum_i y_i \frac{K(x_i,x)}{\sum_jK(x_j,x)}$

也就是：

$\hat \omega(x_i,x) = \frac{K(x_i,x)}{\sum_jK(x_j,x)}$

带宽$b$，高斯核函数：

$e^{-(x-x_i)^2/2b}$

核函数保证靠近预测点权重高，远的降低但不会为0.